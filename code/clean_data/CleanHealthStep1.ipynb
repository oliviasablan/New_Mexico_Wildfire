{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, begin with ESSENCE data\n",
    "##### This data is organized into one dataset per year per query codes (e.g., \"Broad Respiratory 2016\", \"Broad Respiratory 2017\"...)\n",
    "##### For the case crossover analysis, I will need to have columns of each diagnoses with a \"1\" or a \"0,\" so I will create those columns here \n",
    "##### I will also drop duplicated entries and then drop patient's with multiple visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in all years of ESSENCE data files for broad respiratory\n",
    "inputfiles = glob.glob('C:/Users/olivia.sablan/OneDrive - State of New Mexico/Documents/Data/health/ESSENCE/PATIENT_broad*');\n",
    "df_from_each_file = (pd.read_csv(f) for f in inputfiles)\n",
    "# concatenate into one large file \n",
    "df = pd.concat(df_from_each_file, sort = False) \n",
    "# convert date to pandas datetime value\n",
    "df['Date'] = pd.to_datetime(df['Date'], utc = True)\n",
    "# make a new column in this dataframe where Broad Respiratory diagnoses is 1 and all others are 0 \n",
    "df['BroadResp'] = 1 \n",
    "df['AQResp'] = 0\n",
    "df['Asthma'] = 0\n",
    "df['Cardio'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESSENCE has duplicate entries, we can remove these by drop the duplicates of the following three variables:\n",
    "newbroad = df.drop_duplicates(subset = ['Facility Name',  'Visit_ID', 'Medical_Record_Number']) \n",
    "# subset only important variables (trying to make the dataset a bit smaller)\n",
    "newbroad = newbroad [['PIN','Date', 'Time', 'Facility Name',  'Patient_Zip', 'Zipcode', 'Region', 'C_Patient_County', 'Visit_ID', 'Medical_Record_Number', 'Sex', 'EssenceID', 'Age', 'C_Unique_Patient_ID', 'c_ethnicity', 'c_race',\n",
    "                      'HospitalZip', 'Facility', 'Insurance_Coverage', 'Insurance_Company_ID', 'Facility Region', 'FacilityType', 'Hospital_HRSA_County_Designation', 'Year', 'AlternatePatientID', 'Facility_Type_Description', 'Patient_City',\n",
    "                      'BroadResp', 'AQResp', 'Asthma', 'Cardio']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    " # to deal with people that come in often for multiple visits, we will keep the first entry of every patient ID and remove the rest\n",
    "finalbroad =newbroad.drop_duplicates(subset = ['Visit_ID'], keep = 'first')\n",
    "finalbroad.to_csv('../Data/health/ESSENCE/cleanedBroadResp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat the above steps with the other diagnosis queries (Air quality Respiratory here)\n",
    "inputfiles = glob.glob('C:/Users/olivia.sablan/OneDrive - State of New Mexico/Documents/Data/health/ESSENCE/PATIENT_AQ*')\n",
    "df_from_each_file = (pd.read_csv(f) for f in inputfiles)\n",
    "df2 = pd.concat(df_from_each_file, sort = False)\n",
    "df2['Date'] = pd.to_datetime(df2['Date'], utc = True)\n",
    "df2['BroadResp'] = 0\n",
    "df2['AQResp'] = 1\n",
    "df2['Asthma'] = 0\n",
    "df2['Cardio'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate entries\n",
    "newAQ = df2.drop_duplicates(subset = ['Facility Name',  'Visit_ID', 'Medical_Record_Number'])\n",
    "newAQ = newAQ [['PIN','Date', 'Time', 'Facility Name',  'Patient_Zip', 'Zipcode', 'Region', 'C_Patient_County', 'Visit_ID', 'Medical_Record_Number', 'Sex', 'EssenceID', 'Age', 'C_Unique_Patient_ID', 'c_ethnicity', 'c_race',\n",
    "                      'HospitalZip', 'Facility', 'Insurance_Coverage', 'Insurance_Company_ID', 'Facility Region', 'FacilityType', 'Hospital_HRSA_County_Designation', 'Year', 'AlternatePatientID', 'Facility_Type_Description', 'Patient_City',\n",
    "                      'BroadResp', 'AQResp', 'Asthma', 'Cardio']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop multiple visits\n",
    "finalAQ =newAQ.drop_duplicates(subset = ['Visit_ID'], keep = 'first')\n",
    "finalAQ.to_csv('../Data/health/ESSENCE/cleanedAQ.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat for asthma\n",
    "inputfiles = glob.glob('C:/Users/olivia.sablan/OneDrive - State of New Mexico/Documents/Data/health/ESSENCE/PATIENT_asthma*');\n",
    "df_from_each_file = (pd.read_csv(f) for f in inputfiles)\n",
    "df3 = pd.concat(df_from_each_file, sort = False)\n",
    "df3['Date'] = pd.to_datetime(df3['Date'], utc = True)\n",
    "df3['BroadResp'] = 0\n",
    "df3['AQResp'] = 0\n",
    "df3['Asthma'] = 1\n",
    "df3['Cardio'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate entries\n",
    "newasthma = df3.drop_duplicates(subset = ['Facility Name',  'Visit_ID', 'Medical_Record_Number'])s\n",
    "newasthma = newasthma [['PIN','Date', 'Time', 'Facility Name',  'Patient_Zip', 'Zipcode', 'Region', 'C_Patient_County', 'Visit_ID', 'Medical_Record_Number', 'Sex', 'EssenceID', 'Age', 'C_Unique_Patient_ID', 'c_ethnicity', 'c_race',\n",
    "                      'HospitalZip', 'Facility', 'Insurance_Coverage', 'Insurance_Company_ID', 'Facility Region', 'FacilityType', 'Hospital_HRSA_County_Designation', 'Year', 'AlternatePatientID', 'Facility_Type_Description', 'Patient_City',\n",
    "                      'BroadResp', 'AQResp', 'Asthma', 'Cardio']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop multiple vists\n",
    "finalasthma =newasthma.drop_duplicates(subset = ['Visit_ID'], keep = 'first')\n",
    "finalasthma.to_csv('../Data/health/ESSENCE/cleanedAsthma.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lastly, repeat for cardiovascular disease\n",
    "inputfiles = glob.glob('C:/Users/olivia.sablan/OneDrive - State of New Mexico/Documents/Data/health/ESSENCE/PATIENT_CVD*.csv');\n",
    "df_from_each_file = (pd.read_csv(f) for f in inputfiles)\n",
    "df_from_each_file\n",
    "df4 = pd.concat(df_from_each_file, sort = False)\n",
    "df4['Date'] = pd.to_datetime(df4['Date'], utc = True)\n",
    "df4['BroadResp'] = 0\n",
    "df4['AQResp'] = 0\n",
    "df4['Asthma'] = 0\n",
    "df4['Cardio'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate entries\n",
    "newcardio = df4.drop_duplicates(subset = ['Facility Name',  'Visit_ID', 'Medical_Record_Number']) #Drop duplicates of 3 rows\n",
    "newcardio = newcardio [['PIN','Date', 'Time', 'Facility Name',  'Patient_Zip', 'Zipcode', 'Region', 'C_Patient_County', 'Visit_ID', 'Medical_Record_Number', 'Sex', 'EssenceID', 'Age', 'C_Unique_Patient_ID', 'c_ethnicity', 'c_race',\n",
    "                      'HospitalZip', 'Facility', 'Insurance_Coverage', 'Insurance_Company_ID', 'Facility Region', 'FacilityType', 'Hospital_HRSA_County_Designation', 'Year', 'AlternatePatientID', 'Facility_Type_Description', 'Patient_City',\n",
    "                      'BroadResp', 'AQResp', 'Asthma', 'Cardio']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop multiple visits\n",
    "finalcardio =newcardio.drop_duplicates(subset = ['Visit_ID'], keep = 'first')\n",
    "finalcardio.to_csv('../Data/health/ESSENCE/cleanedCardio.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data removed bc duplicates:  0.45\n"
     ]
    }
   ],
   "source": [
    "print('Total data removed bc duplicates: ', round(((len(df3) - len(newasthma)) + (len(df2) - len(newAQ)) + (len(df) - len(newbroad)) +(len(df4)-len(newcardio)))/ (len(df4) + len(df3) + len(df2) + len(df)) *100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data removed bc multiple visits:  0.95\n"
     ]
    }
   ],
   "source": [
    "print('Total data removed bc multiple visits: ', round(((len(df3) - len(finalasthma)) + (len(df2) - len(finalAQ)) + (len(df) - len(finalbroad)) +(len(df4)-len(finalcardio)))/ (len(df4) + len(df3) + len(df2) + len(df)) *100, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, move on to the initial cleaning of Emergency Department visits\n",
    "##### The duplicate entries have already been removed from this dataset, so we just need to remove the patient's with multiple visits for one diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the ED data (this includes all diagnoses)\n",
    "ED_original = pd.read_csv(\"C:/Users/olivia.sablan/OneDrive - State of New Mexico/Documents/Data/health/ED/haquast_data.csv\");\n",
    "# Make a copy so we can compare the cleaned dataset to the original one\n",
    "ED = ED_original.copy(deep = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all the diagnoses column names\n",
    "diagnoses = ['All_respiratory', 'Asthma', 'COPD', 'Pneumonia', 'Bronchitis', 'All_cardiovascular', 'Cardiac_arrest', 'Arrythmia', 'Heart_failure', 'MI', 'Cerebrovascular']\n",
    "# create an empty dataframe\n",
    "removemultipleED = pd.DataFrame()\n",
    "# because we just want to remove multiple visits from each patient per diagnosis and this is a combined dataset of all diagnoses, \n",
    "# we have to loop through and subset each diangoses, remove multiple visits (keeping only the first visit), and then concatenate the cleaned data back together\n",
    "for i in range(len(diagnoses)):\n",
    "    oneoutcome = ED[ED[diagnoses[i]] == 1]\n",
    "    removed = oneoutcome.drop_duplicates(subset = ['Patient_ID'], keep = 'first')\n",
    "    if (i == 0):\n",
    "        removemultipleED = removed\n",
    "    if (i > 0):\n",
    "        removemultipleED = pd.concat([removed, removemultipleED], sort = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total ED data removed bc multiple visits:  0.86\n"
     ]
    }
   ],
   "source": [
    "print('Total ED data removed bc multiple visits: ', round((len(ED) - len(removemultipleED))/ len(ED), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "removemultipleED.to_csv('../Data/health/ED/ED_data_multipleremoved.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
